{
  "slug" : "9603-shell-text-processing-grep-sed-awk",
  "meta" : {
    "slug" : "9603-shell-text-processing-grep-sed-awk",
    "title" : "kae3g 9603: Shell Text Processing - grep, sed, awk Mastery",
    "filename" : "9603-shell-text-processing-grep-sed-awk.md",
    "source-dir" : "hidden"
  },
  "html" : "<h1>kae3g 9603: Shell Text Processing - grep, sed, awk Mastery</h1><p><strong>Phase 2: Core Systems & Tools</strong> | <strong>Week 6</strong> | <strong>Reading Time: 18 minutes</strong><h2></h2></p><h2>What You'll Learn</h2><ul><li>grep: Pattern matching and searching</li><li>sed: Stream editing and text transformation</li><li>awk: Pattern scanning and processing language</li><li>Combining the power trio in pipelines</li><li>Regular expressions essentials</li><li>Real-world text processing workflows</li><li>When to use which tool<h2></h2></li></ul><h2>Prerequisites</h2><ul><li><strong><a href='/12025-10/9550-command-line-your-primary-interface'>9550: Command Line</a></strong> - Pipes, redirection</li><li><strong><a href='/12025-10/9560-text-files-universal-format'>9560: Text Files</a></strong> - Plain text power</li><li><strong><a href='/12025-10/9601-shell-scripting-bash-fundamentals'>9601: Shell Scripting</a></strong> - Shell basics<h2></h2></li></ul><h2>The Power Trio</h2><p><strong>Unix philosophy</strong> (Essay 9510): \"Do one thing well\" + \"Compose tools\"</p><p><strong>grep, sed, awk</strong> are the <strong>text processing champions</strong>:</p><ul><li><strong>grep</strong>: \"Global Regular Expression Print\" - <strong>search and filter</strong></li><li><strong>sed</strong>: \"Stream Editor\" - <strong>search and replace</strong></li><li><strong>awk</strong>: Pattern scanning language - <strong>extract and compute</strong></li></ul><p><strong>Together</strong>: Process terabytes of logs, transform data, extract insights!<h2></h2></p><h2>grep: Search and Filter</h2><h3>Basic Usage</h3><pre><code class=\"bash\"># Search for pattern in file\ngrep &quot;ERROR&quot; /var/log/app.log\n\n# Search in multiple files\ngrep &quot;TODO&quot; &#42;.js\n\n# Recursive search\ngrep -r &quot;function&quot; src/\n\n# Case-insensitive\ngrep -i &quot;warning&quot; log.txt\n</code></pre><h3>Common Options</h3><pre><code class=\"bash\"># -n: Show line numbers\ngrep -n &quot;ERROR&quot; log.txt\n# Output: 42:ERROR: Connection failed\n\n# -v: Invert match &#40;lines NOT matching&#41;\ngrep -v &quot;DEBUG&quot; log.txt  # Exclude debug lines\n\n# -c: Count matches\ngrep -c &quot;ERROR&quot; log.txt  # Output: 15\n\n# -l: List files with matches\ngrep -l &quot;TODO&quot; &#42;.js  # Output: app.js utils.js\n\n# -A, -B, -C: Context lines\ngrep -A 3 &quot;ERROR&quot; log.txt  # Show 3 lines after\ngrep -B 2 &quot;ERROR&quot; log.txt  # Show 2 lines before\ngrep -C 2 &quot;ERROR&quot; log.txt  # Show 2 lines before and after\n</code></pre><h3>Regular Expressions</h3><p><strong>Basic patterns</strong>:</p><pre><code class=\"bash\"># Literal string\ngrep &quot;hello&quot; file.txt\n\n# Start of line &#40;&#94;&#41;\ngrep &quot;&#94;ERROR&quot; log.txt  # Lines starting with ERROR\n\n# End of line &#40;$&#41;\ngrep &quot;failed$&quot; log.txt  # Lines ending with failed\n\n# Any character &#40;.&#41;\ngrep &quot;h.llo&quot; file.txt  # Matches: hello, hallo, h3llo, etc.\n\n# Zero or more &#40;&#42;&#41;\ngrep &quot;colou&#42;r&quot; file.txt  # Matches: color, colour\n\n# One or more &#40;\\+, needs -E&#41;\ngrep -E &quot;10+&quot; file.txt  # Matches: 10, 100, 1000, etc.\n\n# Character class\ngrep &quot;&#91;0-9&#93;&quot; file.txt  # Lines with digits\ngrep &quot;&#91;A-Z&#93;&quot; file.txt  # Lines with uppercase\ngrep &quot;&#91;aeiou&#93;&quot; file.txt  # Lines with vowels\n\n# Word boundary &#40;\\b, needs -E&#41;\ngrep -E &quot;\\bcat\\b&quot; file.txt  # Matches &quot;cat&quot; but not &quot;catch&quot;\n</code></pre><h3>Extended Regex (-E)</h3><pre><code class=\"bash\"># Alternation &#40;|&#41;\ngrep -E &quot;ERROR|FATAL&quot; log.txt  # ERROR or FATAL\n\n# Optional &#40;?&#41;\ngrep -E &quot;colou?r&quot; file.txt  # color or colour\n\n# Groups\ngrep -E &quot;&#40;ab&#41;+&quot; file.txt  # ab, abab, ababab, etc.\n\n# Exactly n times {n}\ngrep -E &quot;&#91;0-9&#93;{3}&quot; file.txt  # Exactly 3 digits\n\n# Range {n,m}\ngrep -E &quot;&#91;0-9&#93;{3,5}&quot; file.txt  # 3 to 5 digits\n</code></pre><h3>Real-World Examples</h3><p><strong>Extract IP addresses</strong>:<pre><code class=\"bash\">grep -Eo &quot;&#91;0-9&#93;{1,3}\\.&#91;0-9&#93;{1,3}\\.&#91;0-9&#93;{1,3}\\.&#91;0-9&#93;{1,3}&quot; access.log\n</code></pre></p><p><strong>Find email addresses</strong>:<pre><code class=\"bash\">grep -Eo &quot;&#91;a-zA-Z0-9.&#95;%+-&#93;+@&#91;a-zA-Z0-9.-&#93;+\\.&#91;a-zA-Z&#93;{2,}&quot; users.txt\n</code></pre></p><p><strong>Filter log levels</strong>:<pre><code class=\"bash\"># Only ERROR and FATAL\ngrep -E &quot;&#94;&#40;ERROR|FATAL&#41;&quot; log.txt\n\n# Everything except DEBUG and INFO\ngrep -Ev &quot;&#94;&#40;DEBUG|INFO&#41;&quot; log.txt\n</code></pre></p><h2></h2><h2>sed: Stream Editing</h2><h3>Basic Substitution</h3><pre><code class=\"bash\"># Replace first occurrence per line\nsed 's/old/new/' file.txt\n\n# Replace all occurrences &#40;g flag&#41;\nsed 's/old/new/g' file.txt\n\n# Edit file in-place\nsed -i 's/old/new/g' file.txt  # Linux\nsed -i '' 's/old/new/g' file.txt  # macOS &#40;requires empty string&#41;\n</code></pre><p><strong>Syntax</strong>: <code>s/pattern/replacement/flags</code></p><h3>Advanced Substitution</h3><pre><code class=\"bash\"># Case-insensitive &#40;I flag&#41;\nsed 's/error/ERROR/gI' file.txt\n\n# Replace only on lines matching pattern\nsed '/&#94;ERROR/s/foo/bar/g' file.txt\n\n# Delete lines\nsed '/DEBUG/d' log.txt  # Delete lines with DEBUG\n\n# Print only matching lines\nsed -n '/ERROR/p' log.txt  # Like grep!\n\n# Multiple commands\nsed -e 's/foo/bar/g' -e 's/baz/qux/g' file.txt\n# Or use semicolon:\nsed 's/foo/bar/g; s/baz/qux/g' file.txt\n</code></pre><h3>Capture Groups</h3><pre><code class=\"bash\"># \\1, \\2 reference captured groups\nsed 's/\\&#40;&#91;0-9&#93;\\{4\\}\\&#41;-\\&#40;&#91;0-9&#93;\\{2\\}\\&#41;-\\&#40;&#91;0-9&#93;\\{2\\}\\&#41;/\\3\\/\\2\\/\\1/' dates.txt\n# Transform: 2025-10-10 → 10/10/2025\n\n# Swap words\necho &quot;hello world&quot; | sed 's/\\&#40;.&#42;\\&#41; \\&#40;.&#42;\\&#41;/\\2 \\1/'\n# Output: world hello\n</code></pre><h3>Line Ranges</h3><pre><code class=\"bash\"># Lines 10-20\nsed -n '10,20p' file.txt\n\n# From line 5 to end\nsed -n '5,$p' file.txt\n\n# Every 3rd line\nsed -n '1&#126;3p' file.txt\n\n# Delete first line\nsed '1d' file.txt\n\n# Delete last line\nsed '$d' file.txt\n</code></pre><h3>Real-World Examples</h3><p><strong>Remove comments</strong>:<pre><code class=\"bash\">sed 's/#.&#42;$//' config.txt  # Remove # to end of line\nsed '/&#94;$/d' config.txt  # Remove blank lines\n</code></pre></p><p><strong>Add line numbers</strong>:<pre><code class=\"bash\">sed = file.txt | sed 'N;s/\\n/\\t/'\n</code></pre></p><p><strong>Extract between markers</strong>:<pre><code class=\"bash\">sed -n '/START/,/END/p' file.txt\n</code></pre></p><p><strong>Config file editing</strong>:<pre><code class=\"bash\"># Change port in nginx config\nsed -i 's/listen 80/listen 8080/' /etc/nginx/nginx.conf\n\n# Enable a commented setting\nsed -i 's/&#94;# &#42;\\&#40;max&#95;connections = \\&#41;/\\1/' config.ini\n</code></pre></p><h2></h2><h2>awk: Pattern Scanning & Processing</h2><h3>Basic Syntax</h3><pre><code class=\"bash\"># Print entire line\nawk '{print}' file.txt\n\n# Print specific field &#40;$1 = first, $2 = second, etc.&#41;\nawk '{print $1}' file.txt\n\n# Print multiple fields\nawk '{print $1, $3}' file.txt\n\n# Field separator &#40;default: whitespace&#41;\nawk -F: '{print $1}' /etc/passwd  # Use : as separator\n</code></pre><h3>Patterns and Actions</h3><pre><code class=\"bash\"># Pattern { action }\nawk '/ERROR/ {print $0}' log.txt  # Like grep\n\n# Only lines &gt; 80 chars\nawk 'length &gt; 80' file.txt\n\n# Print line number and content\nawk '{print NR, $0}' file.txt\n\n# Lines 10-20\nawk 'NR&gt;=10 &amp;&amp; NR&lt;=20' file.txt\n</code></pre><h3>Built-in Variables</h3><pre><code class=\"bash\">NR    # Current line number\nNF    # Number of fields in current line\n$0    # Entire line\n$1    # First field\n$NF   # Last field\nFS    # Field separator &#40;input&#41;\nOFS   # Output field separator\n</code></pre><p><strong>Examples</strong>:</p><pre><code class=\"bash\"># Print last field\nawk '{print $NF}' file.txt\n\n# Print number of fields per line\nawk '{print NF}' file.txt\n\n# Swap first and last fields\nawk '{temp=$1; $1=$NF; $NF=temp; print}' file.txt\n</code></pre><h3>Arithmetic</h3><pre><code class=\"bash\"># Sum column\nawk '{sum += $1} END {print sum}' numbers.txt\n\n# Average\nawk '{sum += $1; count++} END {print sum/count}' numbers.txt\n\n# Min/Max\nawk 'NR==1 {min=$1; max=$1} {if&#40;$1&lt;min&#41; min=$1; if&#40;$1&gt;max&#41; max=$1} END {print min, max}' numbers.txt\n\n# Print with calculation\nawk '{print $1, $2, $1&#42;$2}' data.txt  # Print col1, col2, product\n</code></pre><h3>Conditionals</h3><pre><code class=\"bash\"># if/else\nawk '{if &#40;$1 &gt; 100&#41; print &quot;high&quot;; else print &quot;low&quot;}' numbers.txt\n\n# Ternary operator\nawk '{print &#40;$1 &gt; 100&#41; ? &quot;high&quot; : &quot;low&quot;}' numbers.txt\n\n# Multiple conditions\nawk '$1 &gt; 100 &amp;&amp; $2 &lt; 50 {print}' data.txt\n</code></pre><h3>BEGIN and END</h3><pre><code class=\"bash\"># BEGIN: Runs before processing\n# END: Runs after processing\n\nawk 'BEGIN {print &quot;Starting...&quot;} {print $1} END {print &quot;Done!&quot;}' file.txt\n\n# CSV with header\nawk 'BEGIN {FS=&quot;,&quot;; print &quot;Name,Age&quot;} {print $1, $2}' data.csv\n\n# Statistics\nawk 'BEGIN {count=0; sum=0} {sum+=$1; count++} END {print &quot;Count:&quot;, count, &quot;Average:&quot;, sum/count}' numbers.txt\n</code></pre><h3>Real-World Examples</h3><p><strong>Parse access logs</strong>:<pre><code class=\"bash\"># Extract IP and status code\nawk '{print $1, $9}' access.log\n\n# Count 404 errors\nawk '$9 == 404 {count++} END {print count}' access.log\n\n# Top 10 IPs\nawk '{print $1}' access.log | sort | uniq -c | sort -rn | head -10\n</code></pre></p><p><strong>Process CSV</strong>:<pre><code class=\"bash\"># Extract columns 1 and 3\nawk -F, '{print $1, $3}' data.csv\n\n# Filter rows where column 2 &gt; 100\nawk -F, '$2 &gt; 100 {print}' data.csv\n\n# Convert CSV to TSV\nawk -F, '{print $1 &quot;\\t&quot; $2 &quot;\\t&quot; $3}' data.csv\n# Or:\nawk 'BEGIN {FS=&quot;,&quot;; OFS=&quot;\\t&quot;} {print}' data.csv\n</code></pre></p><p><strong>System monitoring</strong>:<pre><code class=\"bash\"># Disk usage over 80%\ndf -h | awk '$5 &gt; 80 {print $6, $5}'\n\n# Process memory &#40;top 5&#41;\nps aux | awk 'NR&gt;1 {print $6, $11}' | sort -rn | head -5\n\n# Network connections by state\nnetstat -an | awk '/&#94;tcp/ {print $6}' | sort | uniq -c\n</code></pre></p><h2></h2><h2>Combining the Power Trio</h2><h3>Pipeline Patterns</h3><p><strong>grep → sed → awk</strong>:</p><pre><code class=\"bash\"># Extract errors, clean, summarize\ngrep ERROR log.txt | \\\n    sed 's/.&#42;ERROR: //' | \\\n    awk '{count&#91;$0&#93;++} END {for &#40;err in count&#41; print count&#91;err&#93;, err}' | \\\n    sort -rn\n</code></pre><p><strong>Example breakdown</strong>:</p><ol><li><code>grep ERROR</code>: Filter error lines</li><li><code>sed 's/.&#42;ERROR: //'</code>: Remove everything before \"ERROR: \"</li><li><code>awk</code>: Count unique errors</li><li><code>sort -rn</code>: Sort by count (reverse numeric)</li></ol><h3>Real-World Workflows</h3><p><strong>Apache log analysis</strong>:</p><pre><code class=\"bash\"># Top 10 requested URLs\nawk '{print $7}' access.log | \\\n    sort | \\\n    uniq -c | \\\n    sort -rn | \\\n    head -10\n</code></pre><p><strong>Failed login attempts</strong>:</p><pre><code class=\"bash\">grep &quot;Failed password&quot; /var/log/auth.log | \\\n    awk '{print $&#40;NF-3&#41;}' | \\\n    sort | \\\n    uniq -c | \\\n    sort -rn\n</code></pre><p><strong>Extract and transform data</strong>:</p><pre><code class=\"bash\"># From: Name: John, Age: 30\n# To:   John,30\n\ngrep &quot;Name:&quot; data.txt | \\\n    sed 's/Name: \\&#40;.&#42;\\&#41;, Age: \\&#40;.&#42;\\&#41;/\\1,\\2/'\n</code></pre><p><strong>Generate report</strong>:</p><pre><code class=\"bash\">#!/bin/bash\n\necho &quot;=== System Report ===&quot;\necho\n\necho &quot;Top 5 Processes by Memory:&quot;\nps aux | awk 'NR&gt;1 {print $6, $11}' | sort -rn | head -5\necho\n\necho &quot;Disk Usage:&quot;\ndf -h | awk 'NR&gt;1 &amp;&amp; $5+0 &gt; 0 {print $6, $5}'\necho\n\necho &quot;Error Count &#40;last hour&#41;:&quot;\ngrep &quot;$&#40;date +%Y-%m-%d\\ %H&#41;&quot; /var/log/syslog | \\\n    grep -c ERROR\n</code></pre><h2></h2><h2>When to Use Which</h2><h3>grep</h3><p><strong>Best for</strong>:</p><ul><li>Finding files with specific content</li><li>Filtering log entries</li><li>Quick pattern matching</li><li>Binary \"yes/no\" searches</li></ul><p><strong>Example</strong>: \"Which files contain 'TODO'?\"</p><h3>sed</h3><p><strong>Best for</strong>:</p><ul><li>Search and replace</li><li>Line deletion/insertion</li><li>Simple transformations</li><li>In-place file editing</li></ul><p><strong>Example</strong>: \"Change all 'http' to 'https'\"</p><h3>awk</h3><p><strong>Best for</strong>:</p><ul><li>Column extraction</li><li>Arithmetic operations</li><li>Structured data processing</li><li>Complex logic and state</li></ul><p><strong>Example</strong>: \"What's the average of column 3?\"</p><h3>Combination</h3><p><strong>Use together</strong> for multi-stage pipelines:</p><ol><li><strong>grep</strong>: Filter relevant lines</li><li><strong>sed</strong>: Clean/transform</li><li><strong>awk</strong>: Extract and compute<h2></h2></li></ol><h2>Try This</h2><h3>Exercise 1: Log Analysis</h3><p>Given <code>access.log</code>:</p><pre><code>192.168.1.1 - - &#91;10/Oct/2025:13:55:36 -0700&#93; &quot;GET /api/users HTTP/1.1&quot; 200 1234\n192.168.1.2 - - &#91;10/Oct/2025:13:55:37 -0700&#93; &quot;GET /api/posts HTTP/1.1&quot; 404 567\n192.168.1.1 - - &#91;10/Oct/2025:13:55:38 -0700&#93; &quot;POST /api/login HTTP/1.1&quot; 200 890\n</code></pre><p><strong>Tasks</strong>:</p><ol><li>Extract all IP addresses</li><li>Count requests by status code</li><li>Find all 404 errors with URLs<pre><code class=\"bash\"># 1. Extract IPs\nawk '{print $1}' access.log\n\n# 2. Count by status\nawk '{print $9}' access.log | sort | uniq -c\n\n# 3. 404 errors with URLs\nawk '$9 == 404 {print $7}' access.log\n</code></pre><h2></h2></li></ol><h3>Exercise 2: Data Transformation</h3><p>Given <code>users.csv</code>:</p><pre><code>John,Doe,30,Engineer\nJane,Smith,25,Designer\nBob,Johnson,35,Manager\n</code></pre><p><strong>Tasks</strong>:</p><ol><li>Print only first and last names</li><li>Find users over 30</li><li>Convert to JSON format<pre><code class=\"bash\"># 1. First and last names\nawk -F, '{print $1, $2}' users.csv\n\n# 2. Users over 30\nawk -F, '$3 &gt; 30 {print}' users.csv\n\n# 3. Convert to JSON\nawk -F, 'BEGIN {print &quot;&#91;&quot;} {printf &quot;  {\\&quot;first\\&quot;:\\&quot;%s\\&quot;,\\&quot;last\\&quot;:\\&quot;%s\\&quot;,\\&quot;age\\&quot;:%s,\\&quot;role\\&quot;:\\&quot;%s\\&quot;}&quot;, $1,$2,$3,$4; if&#40;NR&lt;3&#41; print &quot;,&quot;; else print &quot;&quot;} END {print &quot;&#93;&quot;}' users.csv\n</code></pre><h2></h2></li></ol><h3>Exercise 3: Configuration Management</h3><p>Given <code>config.txt</code>:</p><pre><code># Database settings\ndb&#95;host=localhost\ndb&#95;port=5432\n# db&#95;user=admin\ndb&#95;pass=secret123\n\n# Server settings\nserver&#95;port=8080\n</code></pre><p><strong>Tasks</strong>:</p><ol><li>Remove all comments</li><li>Extract all uncommented settings</li><li>Change db_port to 5433<pre><code class=\"bash\"># 1. Remove comments\nsed 's/#.&#42;$//' config.txt | sed '/&#94;$/d'\n\n# 2. Uncommented settings\ngrep -v &quot;&#94;#&quot; config.txt | grep &quot;=&quot;\n\n# 3. Change port\nsed 's/db&#95;port=5432/db&#95;port=5433/' config.txt\n</code></pre><h2></h2></li></ol><h2>Best Practices</h2><h3>1. Use Appropriate Tool</h3><pre><code class=\"bash\"># BAD: awk for simple search\nawk '/ERROR/ {print}' log.txt\n\n# GOOD: grep for search\ngrep ERROR log.txt\n\n# BAD: sed for arithmetic\nsed ... # complex expression\n\n# GOOD: awk for arithmetic\nawk '{sum+=$1} END {print sum}' numbers.txt\n</code></pre><h3>2. Quote Regular Expressions</h3><pre><code class=\"bash\"># BAD\ngrep $pattern file.txt  # Shell may expand\n\n# GOOD\ngrep &quot;$pattern&quot; file.txt\ngrep '$1 &gt; 100' data.txt  # Single quotes prevent shell expansion\n</code></pre><h3>3. Test on Sample Data</h3><p><strong>Never run destructive commands</strong> on production data first!</p><pre><code class=\"bash\"># Test first\nsed 's/old/new/g' file.txt | head\n\n# Then apply\nsed -i 's/old/new/g' file.txt\n</code></pre><h3>4. Use -n with sed</h3><pre><code class=\"bash\"># BAD: prints everything twice\nsed '/ERROR/p' log.txt\n\n# GOOD: prints only matches\nsed -n '/ERROR/p' log.txt\n</code></pre><h3>5. Escape Special Characters</h3><p><strong>Regex special chars</strong>: <code>. &#42; &#91; &#93; &#94; $ \\ + ? { } | &#40; &#41;</code></p><pre><code class=\"bash\"># Search for literal dot\ngrep &quot;\\.&quot; file.txt\n\n# Search for literal dollar sign\ngrep &quot;\\$&quot; file.txt\n</code></pre><h2></h2><h2>Going Deeper</h2><h3>Related Essays</h3><ul><li><strong><a href='/12025-10/9550-command-line-your-primary-interface'>9550: Command Line</a></strong> - Pipes, redirection</li><li><strong><a href='/12025-10/9601-shell-scripting-bash-fundamentals'>9601: Shell Scripting</a></strong> - Scripting basics</li><li><strong><a href='/12025-10/9603-shell-functions-modularity'>9603: Shell Functions</a></strong> - Reusable text processing (Coming Soon!)</li></ul><h3>External Resources</h3><ul><li><strong>\"sed & awk\"</strong> - Dale Dougherty (O'Reilly, definitive guide)</li><li><strong>Regular-Expressions.info</strong> - Comprehensive regex tutorial</li><li><strong>awk manual</strong> - <code>man awk</code> or GNU awk guide</li><li><strong>\"The AWK Programming Language\"</strong> - Aho, Kernighan, Weinberger<h2></h2></li></ul><h2>Reflection Questions</h2><ol><li><strong>Why three tools instead of one?</strong> (Unix philosophy - simple, composable)</li><li><strong>When is awk overkill?</strong> (Simple search/replace - use grep/sed)</li><li><strong>Can awk replace Python?</strong> (For text processing pipelines, often yes! For apps, no.)</li><li><strong>Why learn regex?</strong> (Universal - works in grep, sed, awk, vim, Python, JavaScript, ...)</li><li><strong>How does this relate to functional programming?</strong> (Pipelines = function composition! <code>grep | sed | awk</code> = <code>compose&#40;awk, sed, grep&#41;</code>)<h2></h2></li></ol><h2>Summary</h2><p><strong>The Power Trio</strong>:</p><ul><li><strong>grep</strong>: Search and filter (<code>grep &quot;pattern&quot; file</code>)</li><li><strong>sed</strong>: Search and replace (<code>sed 's/old/new/g' file</code>)</li><li><strong>awk</strong>: Extract and compute (<code>awk '{sum+=$1} END {print sum}' file</code>)</li></ul><p><strong>grep essentials</strong>:</p><ul><li><code>-i</code>: Case-insensitive</li><li><code>-v</code>: Invert match</li><li><code>-n</code>: Line numbers</li><li><code>-r</code>: Recursive</li><li><code>-E</code>: Extended regex</li></ul><p><strong>sed essentials</strong>:</p><ul><li><code>s/pattern/replacement/g</code>: Substitute</li><li><code>/pattern/d</code>: Delete lines</li><li><code>-n '/pattern/p'</code>: Print matches only</li><li><code>-i</code>: In-place editing</li></ul><p><strong>awk essentials</strong>:</p><ul><li><code>{print $1}</code>: Print first field</li><li><code>$1 &gt; 100 {print}</code>: Conditional action</li><li><code>{sum+=$1} END {print sum}</code>: Accumulate</li><li><code>-F,</code>: Set field separator</li></ul><p><strong>When to use</strong>:</p><ul><li><strong>grep</strong>: Filter lines</li><li><strong>sed</strong>: Transform lines</li><li><strong>awk</strong>: Process columns</li></ul><p><strong>Combine in pipelines</strong>:<pre><code class=\"bash\">grep ERROR log.txt | sed 's/.&#42;ERROR: //' | awk '{count&#91;$0&#93;++} END {for&#40;e in count&#41; print count&#91;e&#93;, e}' | sort -rn\n</code></pre></p><p><strong>In the Valley</strong>:</p><ul><li><strong>Text processing = data flowing through ecosystem</strong></li><li><strong>grep, sed, awk = different plant species (complementary niches!)</strong></li><li><strong>Pipelines = nutrient cycles (output of one feeds input of next)</strong></li><li><strong>Ecological lens</strong>: \"Diverse tools create resilient workflows—monoculture (one tool) is fragile, polyculture (grep+sed+awk) is robust.\"<h2></h2></li></ul><p><strong>Next</strong>: <strong>Essay 9603 - Shell Functions & Modularity!</strong> We'll learn to write reusable, maintainable shell code!<h2></h2></p><p><strong>Navigation</strong>:<br /> ← Previous: <a href='/12025-10/9601-shell-scripting-bash-fundamentals'>9601 (Shell Scripting Fundamentals)</a> | <strong>Phase 2 Index</strong> | Next: <a href='/12025-10/9603-shell-functions-modularity'>9603 (Shell Functions & Modularity)</a> <em>(Coming Soon!)</em></p><p><strong>Metadata</strong>:</p><ul><li><strong>Phase</strong>: 2 (Core Systems & Tools)</li><li><strong>Week</strong>: 6 (Shell Scripting)</li><li><strong>Prerequisites</strong>: 9550, 9560, 9601</li><li><strong>Concepts</strong>: grep (search), sed (transform), awk (process), regex, pipelines</li><li><strong>Next Concepts</strong>: Shell functions, modularity, reusable code</li><li><strong>Plant Lens</strong>: Diverse tools (polyculture), nutrient cycles (pipelines), resilient workflows</li><li><strong>Hands-On</strong>: 3 exercises (log analysis, data transformation, config management)</li></ul><h2></h2><p><div style=\"text-align: center; opacity: 0.6; font-size: 0.85em; margin-top: 3em; padding-top: 1em; border-top: 1px solid rgba(139, 116, 94, 0.2);\"></p><p><strong>Copyright © 2025 <a href='https://codeberg.org/kae3g/12025-10/'>kae3g</a></strong> | Dual-licensed under <a href='https://www.apache.org/licenses/LICENSE-2.0'>Apache-2.0</a> / <a href='https://opensource.org/licenses/MIT'>MIT</a><br /> Competitive technology in service of clarity and beauty</p><p></div></p><p><em><a href='/12025-10/hidden-docs-index.html'>View Hidden Docs Index</a></em> | <em><a href='/12025-10/'>Return to Main Index</a></em></p>",
  "hash" : "2025-10-22T15:17:39.429759612Z-16564"
}